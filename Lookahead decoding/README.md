# LookAhead decoding

- Using this technique, we can make any LLM run faster.
- The LLMs can go upto 2x spped up.
- After testing on latest models such as Llama3, Llama2, Mistral 7b, I can confirm that this technique works.


#### After you run the `main.py` file, you should get something like this:

`Greedy Generated Tokens: 256 Generation Speed: 44 tokens/s`

`Sample Generated Tokens: 256 Generation Speed: 43 tokens/s `

Following we can conclude after this:
#### This demonstrates the **1.6x throughput increase** you were expecting.(This result is based on llama 2b)
